Этот проект создан для демонстрации моих знаний в docker, SQL, взаимодействии между питоновской библиотекой SQLAlchemy и базой данных в PostgreSQL.
Запущу сервер PostgreSQL в docker-контейнере. В нём создам базу данных market и таблицу books_program. В таблицу запишу данные от парсинга магазина wildberries.ru, что он выдаёт по поиску "книги программирование", используя библиотеку SQLAlchemy. Сделаю несколько запросов к бд на языке SQL (диалект postgresql).

Содержание:
- [Docker](#docker)
- [Python](#python)
- [SQL](#sql)
- [SQLAlchemy Pandas](#alchemy)

<a name="docker"><h3 align=center>Docker</h3>
Как уже написал, буду запускать сервер PostgreSQL в docker-контейнере.
Для начала отключу сервер PostgreSQL на хосте, он автоматически у меня запускается как служба в Ubuntu 20.04:
```
sudo service postgresql stop
```
После этого создам имитацию удалённого сервера PostgreSQL:
```
docker run --rm -e POSTGRES_PASSWORD=passw -e POSTGRES_USER=dmitriy 
-e POSTGRES_DB=postgres -d -p 5432:5432 -v $HOME/docker/volumes/postgres:/var/lib/postgresql/data postgres
```
Образы можно создавать с помощью файла Dockerfile, но поскольку мне в контейнере нужен только PostgreSQL, ничего дополнительно устанавливать не надо, то проще всего взять готовый образ с Docker хаба и с нужными опциями запустить контейнер сразу. Опция POSTGRES_PASSWORD является обязательной, POSTGRES_DB необязательная, но поскольку бд dmitriy не существует, то указал postgres, автоматически создаваемую во всех PostgreSQL.

Опция -v служит для проброса каталога с хоста на каталог файловой системы контейнера. Указал, чтобы созданная и заполненная потом на 6000 строк база данных не пропала после удаления контейнера. Контейнер удаляется автоматически из-за опции - rm.

Сервер PostgreSQL создался в контейнере, это как бы отдельный компьютер, имеющий свой собственный IP-адрес (псевдоним localhost). Для связи с этим сервером служат клиентские программы типа pgAdmin или DBeawer, но я предпочитаю пользоваться утилитой psql.
```
psql -h localhost -U dmitriy -d demo
```
demo - конкретно существующая бд в моём запущенном сервере. Неважно какая бд, лишь бы существовала. Всегда существует postgres.
Связь с сервером установили.
Теперь с помощью запросов SQL можно работать с PostgreSQL. Также в psql есть метакоманды. Список некоторых метакоманд:
- \l - список существующих баз данных.
- \dt - список таблиц в базе данных, с которой установили соединение.
- \c - команда для перехода в другую базу данных.

Создам базу данных market. Вообще я не хотел её через SQL создавать, хотел через питоновский скрипт создать. Но на практике, когда я запускал сервер с монтировкой каталогов и с -e POSTGRES_DB=market и потом запускал скрипт.py, то получал исключение, что база данных market не существует. Поэтому:
```
CREATE DATABASE market;
```

Свой путь в программирование, я начинал с изучения робототехники. И на этом этапе мне приходилось писать Dockerfile'ы для запуска gazebo в контейнере, ros2 в хостовой системе ubuntu 20.04 . Приходилось писать и docker-compose.yml . Минимальный опыт есть.
<a name="python"><h3 align=center>Python</h3>
Перейдем теперь к созданию скрипта на языке Python. С помощью этого скрипта буду осуществлять парсинг сайта wildberries.py . С сайта нужно "забрать" все книги, выдаваемые через поиск по словам "книги программирование". Этот скрипт назвал wb.py .

Пытался забрать информацию с стартовой страницы используя библиотеку bs4, не смог. Пришлось использовать Selenium, используемую для запуска браузера.
#### Функции wb.py #
Функция settings_browser() устанавливает настройки браузера через класс ChromeOptions(). Без этих настроек не удастся одолеть защиту сайта от ботов.

Функция open_page() открывает страницу сайта. У меня включена options.add_argument("--headless"), поэтому браузер работает в фоновом режиме, за его работой наблюдаю в терминале. После того как полностью страница полностью прокрутится, все элементы-книги загрузятся, в список записываем словари, где ключи словаря:
- title - название книги
- url - url страницы книги
- price - цена книги
- img_url - url страницы с картинкой обложки книги

 Сформировали список для страницы, смотрим есть ли еще страница. Если есть, переходим на неё с помощью этой же функции (рекурсия). Этой функции передаём список данных, полученных с предыдущих страниц. И когда переходим на последнюю страницу и обработаем её, то получим в итоге список из данных от всех пройденных страниц. Этот конечный список возвращаем через return.
На практике пришлось обработать ровно 60 страниц, где каждая страница - это 100 элементов-книг. Таким образом получил список из 6000 словарей.

С помощью функции write_to_xlsx() записываем данные с списка в файл books_wb.xlsx . Функция является асинхронной, точнее асинхронно работает внутренняя функция write_webp(), которая асинхронно посылает 6000 запросов на чтение картинок с их url. Конечно такое количество запросов возвращает много исключений типа time_out_connection. К тому же когда я открыл books_wb.xlsx, то не смог до конца прокрутить электронную таблицу. Идею с записью такого количества данных в файл для **моего компьютера** признал плохой и потому закомментировал строку asyncio.run(write_to_xlsx(lst_data)) в функции main().

Далее в main() импортирую мой модуль database и выполняю из этого модуля функцию write_db() . По названию функции понятно, что она записывает данные из списка в базу данных сервера PostgreSQL. Для этого используется кросс-диалектная библиотека SQLAlchemy.
Чтобы данные в таблице books_program не дублировались каждый раз с запуском wb.py, реализовал предварительное очищение таблицы.
 В результате работы скрипта wb.py в базе данных market появится таблица books_program. 
<a name="sql"><h3 align=center>SQL</h3>
Теперь можно вернуться к разделу, посвященному Docker. После того, как создали сервер PostgreSQL через утилиту psql сразу устанавливаю соединение с market:
```
psql -h localhost -U dmitriy -d market
```
Смотрю количество строк в books_program:
```
SELECT count(id) FROM books_program;
```
![Изображение](images/1.png)

А также уникальных по названию книги.
Как видно, в таблице очень много дублей. Всего записей 6000, но уникальных 2503. Мне это не нравится и попробую создать новую таблицу на базе books_program, которая будет содержать уникальные title, минимальную цену и url, url_img соответствующие строке с минимальной ценой. Как же это сделать?
Запрос 1:
```
SELECT left_table.title, left_table.price, right_table.url
FROM (SELECT title AS title, MIN(price) AS price
      FROM books_program
      GROUP BY title
     ) AS left_table
JOIN
     (SELECT title, price, url
      FROM books_program
     ) AS right_table
ON left_table.title = right_table.title AND left_table.price = right_table.price
LIMIT 10;
```
![Изображение](images/2.png)

Создадим на основе этого запроса новую таблицу unik_books_program по структуре повторяющую  books_program.
Но решил, что нужна еще одна таблица, выполняющая функции справочника книг books, которая будет содержать только 2 столбца: id - идентификатор книги и title - название книги. Смысл этой таблицы - являться справочником по названиям книг.
```
CREATE TABLE books
(id SERIAL PRIMARY KEY, 
 title text UNIQUE NOT NULL);
```
Вставим в эту таблицу уникумы по title из books_program.
```
INSERT INTO books (title)
SELECT DISTINCT title FROM books_program;
```
![Изображение](images/3.png)

Теперь займёмся таблицей unik_books_program.
```
CREATE TABLE unik_books_program
(
  id SERIAL PRIMARY KEY,
  title text NOT NULL,
  price INTEGER,
  url varchar(80),
  url_img varchar(100),
  books_id INTEGER REFERENCES books (id)
);
```
Поле books_id является внешним ключом и связана с родительским ключом id таблицы books.
Вставка значений из books_program на основе первоначального запроса 1 будет неправильной, поскольку теперь нужно учитывать books_id, связанный с id из books.
```
INSERT INTO unik_books_program (title, price, url, url_img, books_id)
SELECT left_table.title, right_table.price, url, url_img, id
FROM (
       (SELECT * FROM books) AS left_table
       JOIN
       (
        SELECT DISTINCT ON (title) title, price, url, url_img
        FROM books_program
        ORDER BY title, price
       ) AS right_table
       ON left_table.title = right_table.title
     );
```
![Изображение](images/4.png)

Здесь очень помог существующий только в PostgreSQL оператор DISTINCT ON. Без него получился бы довольно громоздкий запрос. В операторе FROM сформировали объединённую таблицу из books с right_table. Right_table сформирована из books_program, только столбец title содержит уникальные title из books_program, столбец price отсортирован по возрастанию, поэтому когда извлекаем из группы title первую запись, то это означает, что берём минимальную цену и соответствующие url и url_img. Всё, как я хотел.

Получил требуемые таблицы books и unik_books_program. На этом работу с SQL закончу. Думаю, я знаю SQL на хорошем уровне.
<a name="alchemy"><h3 align=center>SQLAlchemy Pandas</h3>
Теперь сделаю поиск из unik_books_program по списку слов ['нейро', 'ai', ' ml', 'аналит', 'робототехник'] и запишу выборку в файл pandas.xlsx.

Скрипт называется alchemy.py .
В первой части через SQLAlchemy я устанавливаю соединение (движок) с market. Делаю запросы к таблице unik_books_program и результат запроса записываю в список lst_results.

Во второй части, я этот список запишу в файл pandas.xlsx, причём вместо url_img будет вставлена картинка, причем картинка эта в нечитаемом формате webp, поэтому ещё придётся делать конвертирование в png. Поскольку количество картинок в результате запросов небольшое, то запрашивать их буду последовательно, одна за другой (синхронно).

Обычно для работы с электронными таблицами, я применяю библиотеку openpyxl, но здесь для демонстрации знаний применю pandas.
Скрипт получился не большой, легко читается.
С SQLAlchemy пришлось иметь дело впервые, пришлось почитать документацию. С pandas также не помню, чтобы использовал его в проектах.

На этом всё.
